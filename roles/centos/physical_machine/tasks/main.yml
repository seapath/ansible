# We need to restart nginx if it's running and know if gunicorn socket is present
- name: Populate service facts
  service_facts:
#- name: Print service facts
#  ansible.builtin.debug:
#    var: ansible_facts.services
- name: install vm-mgr http interface
  vars:
    vmmgrapi_certs_dir: "/var/local/vmmgrapi/certs"
  block:
    - name: create vm-mgr api certs folder
      file:
        path: "{{ vmmgrapi_certs_dir }}"
        state: directory
        mode: 0755

    - name: upload cert/key if provided
      copy:
        src: "{{ item }}"
        dest: "{{ vmmgrapi_certs_dir }}/{{ item }}"
        mode: '0644'
      with_items:
        - "{{ vmmgr_http_tls_crt_path }}"
        - "{{ vmmgr_http_tls_key_path }}"
      when:
        - vmmgr_http_tls_crt_path is defined
        - vmmgr_http_tls_key_path is defined

    - name: create certificat / key if not provided
      command: openssl req -x509 -nodes -days 9125 -newkey rsa:4096 -subj "/C=FR/ST=seapath/L=seapath/O=seapath/OU=seapath/CN=seapath" -keyout "{{ vmmgrapi_certs_dir }}/seapath.key" -out "{{ vmmgrapi_certs_dir }}/seapath.crt"
      args:
        creates: "{{ item }}"
      with_items:
        - "{{ vmmgrapi_certs_dir }}/seapath.crt"
        - "{{ vmmgrapi_certs_dir }}/seapath.key"

    - name: Correct certificates rights
      file:
        path: "{{ vmmgrapi_certs_dir }}/{{ item }}"
        mode: 0644
      loop:
        - "seapath.crt"

    - name: Correct private keys rights
      file:
        path: "{{ vmmgrapi_certs_dir }}/{{ item }}"
        mode: 0640
      loop:
        - "seapath.key"

    - name: Check permission on authentication file
      ansible.builtin.file:
        path: "{{ vmmgr_http_local_auth_file }}"
        owner: www-data
        group: www-data
        mode: '0600'
        state: touch
      when: vmmgr_http_local_auth_file is defined

    - name: Copy nginx.conf
      template:
        src: ../src/debian/vmmgrapi/nginx.conf.j2
        dest: /etc/nginx/nginx.conf
        mode: '0600'
      register: nginx_conf

    - name: restart nginx if needed
      ansible.builtin.systemd:
        name: nginx.service
        enabled: no
        state: stopped
      when:
        - nginx_conf.changed
        - services['nginx.service']['state'] == "running"

    - name: disable nginx
      ansible.builtin.systemd:
        name: nginx.service
        enabled: no

    - name: Copy vmmgrapi files
      ansible.builtin.copy:
        src: ../src/debian/vmmgrapi/{{ item }}
        dest: /var/local/vmmgrapi/{{ item }}
        mode: '0644'
      with_items:
        - wsgi.py

    - name: Copy vmmgrapi systemd files
      ansible.builtin.copy:
        src: ../src/debian/vmmgrapi/{{ item }}
        dest: /etc/systemd/system/{{ item }}
        mode: '0644'
      with_items:
        - gunicorn.socket
        - gunicorn.service
      register: vmmgrapi_systemd

    - name: daemon-reload vmmgrapi
      ansible.builtin.service:
        daemon_reload: yes
      when: vmmgrapi_systemd.changed

    - name: restart gunicorn.socket if needed
      ansible.builtin.systemd:
        name: gunicorn.socket
        enabled: yes
        state: restarted
      when: vmmgrapi_systemd.changed

    - name: start and enable gunicorn.socket
      ansible.builtin.systemd:
        name: gunicorn.socket
        enabled: yes
        state: started

  when: enable_vmmgr_http_api is defined and enable_vmmgr_http_api is true

- name: disable gunicorn.socket if http flask api is not enabled
  ansible.builtin.systemd:
    name: gunicorn.socket
    enabled: no
    state: stopped
  when:
    - enable_vmmgr_http_api is not defined or enable_vmmgr_http_api is false
    - services['gunicorn.socket'] is defined

- name: disable gunicorn.service all the time, if it exists
  ansible.builtin.systemd:
    name: gunicorn.service
    enabled: no
  when:
    - services['gunicorn.service'] is defined

- name: Copy sysctl rules
  ansible.builtin.copy:
    src: ../src/debian/sysctl/{{ item }}
    dest: /etc/sysctl.d/{{ item }}
    mode: '0644'
  with_items:
    - 00-bridge_nf_call.conf
  register: sysctl1

- name: Add sysctl conf from inventory (extra_sysctl_physical_machines)
  ansible.builtin.copy:
    dest: /etc/sysctl.d/00-seapathextra_physicalmachines.conf
    mode: '0644'
    content: "{{ extra_sysctl_physical_machines }}"
  when: extra_sysctl_physical_machines is defined
  register: sysctl2

- name: restart systemd-sysctl if needed
  ansible.builtin.systemd:
    name: systemd-sysctl.service
    state: restarted
  when: sysctl1.changed or sysctl2.changed

- name: create src folder on hosts
  file:
    path: /tmp/src
    state: directory
    mode: '0755'

- name: Synchronization of src python3-setup-ovs on the control machine to dest on the remote hosts
  ansible.posix.synchronize:
    src: ../src/debian/python3-setup-ovs
    dest: /tmp/src
    rsync_opts:
      - "--chown=root:root"
- name: Install python3-setup-ovs
  command:
    cmd: /usr/bin/python3 setup.py install
    chdir: /tmp/src/python3-setup-ovs
- name: Copy seapath-config_ovs.service
  ansible.builtin.copy:
    src: ../src/debian/seapath-config_ovs.service
    dest: /etc/systemd/system/seapath-config_ovs.service
    mode: '0644'
  register: seapathconfigovs
- name: daemon-reload seapath-config_ovs
  ansible.builtin.service:
    daemon_reload: yes
  when: seapathconfigovs.changed
- name: enable seapath-config_ovs.service
  ansible.builtin.systemd:
    name: seapath-config_ovs.service
    enabled: yes

- name: Synchronization of src vm_manager on the control machine to dest on the remote hosts
  ansible.posix.synchronize:
    src: ../src/debian/vm_manager
    dest: /tmp/src
    rsync_opts:
      - "--chown=root:root"
- name: Install vm_manager
  command:
    cmd: /usr/bin/python3 setup.py install
    chdir: /tmp/src/vm_manager
- name: Create a symbolic link
  ansible.builtin.file:
    src: /usr/local/bin/vm_manager_cmd.py
    dest: /usr/local/bin/vm-mgr
    state: link

- name: Copy consolevm.sh
  template:
    src: ../src/debian/consolevm.sh.j2
    dest: /usr/local/bin/consolevm
    mode: '0755'

- name: Synchronization of backup-restore folder on the control machine to dest on the remote hosts
  ansible.posix.synchronize:
    src: ../src/debian/backup-restore/
    dest: /usr/local/bin/
    rsync_opts:
      - "--chown=root:root"
      - "--exclude=*.j2"
- name: Copy backup-restore templates
  template:
    src: "{{ item }}"
    dest: /usr/local/bin/{{ item | basename | regex_replace('\.j2$', '') }}
    mode: '0755'
  with_fileglob:
    - ../src/debian/backup-restore/*.j2
- name: create /etc/backup-restore.conf file
  file:
    path: "/etc/backup-restore.conf"
    state: touch
    mode: 0644
    owner: root
    group: root
- name: check configuration of backup-restore.sh tool (remote_shell)
  shell: 'grep -c "^remote_shell=" /etc/backup-restore.conf || true'
  register: check_remote_shell
- name: add default configuration of backup-restore.sh tool (remote_shell)
  lineinfile:
    dest: /etc/backup-restore.conf
    regexp: "^remote_shell="
    line: "remote_shell=\"ssh\""
    state: present
  when: check_remote_shell.stdout == "0"

- name: create /usr/lib/ocf/resource.d/seapath on hosts
  file:
    path: /usr/lib/ocf/resource.d/seapath
    state: directory
    mode: '0755'

- name: Copy Pacemaker Seapath Resource-Agent files
  ansible.posix.synchronize:
    src: ../src/debian/pacemaker_ra/
    dest: /usr/lib/ocf/resource.d/seapath/
    rsync_opts:
    - "--chmod=F755"
    - "--chown=root:root"

- name: Copy chrony-wait.service
  template:
    src: ../src/debian/chrony-wait.service.j2
    dest: /etc/systemd/system/chrony-wait.service
    owner: root
    group: root
    mode: '0644'
  register: chronywait
- name: daemon-reload chrony-wait.service
  ansible.builtin.service:
    daemon_reload: yes
  when: chronywait.changed
- name: enable chrony-wait.service
  ansible.builtin.systemd:
    name: chrony-wait.service
    enabled: yes

- name: Create libvirtd.service.d directory
  file:
    path: /etc/systemd/system/libvirtd.service.d/
    state: directory
    owner: root
    group: root
    mode: 0755
- name: Create pacemaker.service.d directory
  file:
    path: /etc/systemd/system/pacemaker.service.d/
    state: directory
    owner: root
    group: root
    mode: 0755
- name: Copy pacemaker.service drop-in
  template:
    src: ../src/debian/pacemaker_override.conf.j2
    dest: /etc/systemd/system/pacemaker.service.d/override.conf
    owner: root
    group: root
    mode: 0644
  notify: daemon-reload
  register: pacemaker_corosync
- name: Get Pacemaker service Status
  ansible.builtin.systemd:
    name: "pacemaker.service"
  register: pacemaker_service_status
- name: disable pacemaker (reinstall step 1/2)
  ansible.builtin.systemd:
    name: pacemaker.service
    enabled: no
  when: pacemaker_corosync.changed and pacemaker_service_status.status.UnitFileState == "enabled"
- name: enable pacemaker (reinstall step 2/2)
  ansible.builtin.systemd:
    name: pacemaker.service
    enabled: yes
  when: pacemaker_corosync.changed and pacemaker_service_status.status.UnitFileState == "enabled"

- name: Add extra modules to the kernel
  block:
    - name: create extra modules conf file
      ansible.builtin.file:
        path: /etc/modules-load.d/extra_modules.conf
        owner: root
        group: root
        mode: 0751
        state: touch
    - name: add extra modules to conf file
      lineinfile:
        dest: /etc/modules-load.d/extra_modules.conf
        state: present
        regexp: "^{{ item }}$"
        line: "{{ item }}"
      with_items: "{{ extra_kernel_modules | default([]) }}"

- name: Add admin user to libvirt and haclient groups
  user:
    name: "{{ admin_user }}"
    groups: libvirt,haclient
    append: yes
- name: Adding live-migration user
  user:
    name: "{{ livemigration_user }}"
    shell: /bin/bash
    group: libvirt
    uid: 1001
    append: no
  when: livemigration_user is defined and livemigration_user != admin_user

- name: get Centos-snmp uid
  getent:
    database: passwd
    key: "Centos-snmp"
- name: get Centos-snmp gid
  getent:
    database: group
    key: "Centos-snmp"
- debug:
    msg:
      - "user id {{ getent_passwd['Centos-snmp'][1] }}"
      - "group id {{ getent_group['Centos-snmp'][1] }}"

- name: Set Centos-snmp correct uid/gid
  block:
  - name: stop snmpd if needed
    ansible.builtin.systemd:
      name: snmpd.service
      state: stopped
  - name: Ensure group "Centos-snmp" exists with correct gid
    ansible.builtin.group:
      name: Centos-snmp
      state: present
      gid: 902
  - name: Ensure user Centos-snmp has correct uid and gid
    user:
      name: Centos-snmp
      uid: 902
      group: Centos-snmp
    when: getent_passwd['Centos-snmp'][1] != "902" or getent_group['Centos-snmp'][1] != "902"

  - name: Synchronization of src snmp_passpersist python module
    ansible.posix.synchronize:
      src: ../src/snmp_passpersist
      dest: /tmp/src
      rsync_opts:
        - "--chown=root:root"
  - name: Install snmp_passpersist
    command:
      cmd: /usr/bin/python3 setup.py install
      chdir: /tmp/src/snmp_passpersist

  - name: Synchronization of snmp_ scripts
    ansible.posix.synchronize:
      src: ../src/debian/snmp/
      dest: /usr/local/bin/
      rsync_opts:
      - "--include=virt-df.sh"
      - "--exclude=*"
      - "--chmod=F755"
      - "--chown=root:root"
  - name: Snmp config
    ansible.builtin.template:
      src: ../src/debian/snmp/snmpd.conf.j2
      dest: /etc/snmp/snmpd.conf
      mode: '0644'
      owner: root
      group: root
    register: snmpd_conf
  - name: SNMP PASS AGENT script run by net-snmp for seapath tree
    copy:
      src: ../src/debian/snmp/exposeseapathsnmp.py
      dest: "/usr/local/bin/exposeseapathsnmp.py"
      mode: '0755'
  - name: script run by cron job to generate snmp data
    copy:
      src: ../src/debian/snmp/snmp_getdata.py
      dest: "/usr/local/sbin/snmp_getdata.py"
      mode: '0755'
  - name: cron job to generate snmp data
    copy:
      src: ../src/debian/snmp/seapathsnmp.cron
      dest: "/etc/cron.d/seapathsnmp"
      mode: '0644'
  - name: Wait for DHCP for SNMP
    lineinfile:
      dest: /lib/systemd/system/snmpd.service
      regexp: "^After="
      line: "After=network-online.target"
      state: present

  - name: restart snmpd
    ansible.builtin.systemd:
      name: snmpd.service
      state: restarted
      enabled: yes

  - name: Install sudo Centos-snmp user rules
    template:
      src: ../src/centos/sudoers/Centos-snmp.j2
      dest: /etc/sudoers.d/Centos-snmp
      owner: root
      group: root
      mode: '0440'

- name: Creating libvirt user with libvirtd permissions
  user: name=libvirt
    group=libvirt
    shell=/bin/false

- name: add br_netfilter to /etc/modules-load.d
  ansible.builtin.copy:
    src: ../src/debian/modules/netfilter.conf
    dest: /etc/modules-load.d/netfilter.conf
    owner: root
    group: root
    mode: 0751

- name: lineinfile in hosts file for logstash-seapath
  lineinfile:
    dest: /etc/hosts
    regexp: '.* logstash-seapath$'
    line: "{{ logstash_server_ip }} logstash-seapath"
    state: present
  when: logstash_server_ip is defined

- name: Make libvirt use the "machine-id" way to determine host UUID
  lineinfile:
    dest: /etc/libvirt/libvirtd.conf
    regexp: "^#?host_uuid_source ="
    line: "host_uuid_source = \"machine-id\""
    state: present
- name: restart libvirtd
  ansible.builtin.systemd:
    name: libvirtd.service
    state: restarted

- name: enable virtsecretd
  ansible.builtin.systemd:
    name: virtsecretd.service
    enabled: yes
    state: started


- name: enable docker.service
  ansible.builtin.systemd:
    name: docker.service

- name: "add lvm_snapshot_rebooter: script file"
  block:
    - name: dracut module dir
      file:
        path: /usr/lib/dracut/modules.d/75lvm-snapshot-rebooter
        state: directory
        mode: 0755
    - name: Copy lvm_snapshot_rebooter script
      ansible.builtin.copy:
        src: ../src/debian/lvm_snapshot_rebooter.sh
        dest: /usr/lib/dracut/modules.d/75lvm-snapshot-rebooter/lvm_snapshot_rebooter
        mode: '0755'
      register: lvm_snapshot_rebooter

- name: "add udev rules for lvm2 limitation"
  ansible.builtin.copy:
    src: ../src/debian/69-lvm.rules
    dest: /etc/udev/rules.d/69-lvm.rules
    mode: '0644'
  when: ansible_distribution == 'Debian' and ansible_distribution_version | int >= 12
  register: udevlvm
- name: "rebuild initramfs if necessary"
  command:
    cmd: /usr/bin/dracut --regenerate-all --force
  when: udevlvm.changed or lvm_snapshot_rebooter.changed

- name: "add rbd type to lvm.conf"
  ansible.builtin.lineinfile:
    path: /etc/lvm/lvm.conf
    insertafter: 'devices {'
    line: "        types = [ \"rbd\", 1024 ]"
    state: present

- name: "Configure firewalld for ceph"
  block:
    - name: "Configure firewalld for ceph monitor"
      ansible.posix.firewalld:
        port: 3300/tcp
        permanent: true
        state: enabled
    - name: "Configure firewalld for ceph monitor legacy v1 port"
      ansible.posix.firewalld:
        port: 6789/tcp
        permanent: true
        state: enabled
    - name: "Configure firewalld for ceph OSD"
      ansible.posix.firewalld:
        port: 6800-7300/tcp
        permanent: true
        state: enabled

- name: "Configure firewalld for high-availability services"
  ansible.posix.firewalld:
    service: high-availability
    permanent: true
    state: enabled
