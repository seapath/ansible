# Copyright (C) 2025 RTE
# SPDX-License-Identifier: Apache-2.0

---
- include_vars: "{{ ansible_os_family }}.yml"

# === User/Group setup (SEAPATH-specific) ===
- name: Set up cephadm user and group
  include_tasks: setup_user.yml

# === Cluster detection ===
- name: Check if host is part of a Ceph cluster
  command: "ceph -s"
  register: cephadm_ceph_status
  ignore_errors: true
  changed_when: false

- name: Mark host as ceph or nonceph
  set_fact:
    cephadm_is_ceph_node: "{{ cephadm_ceph_status.rc == 0 }}"

- name: Add host to ceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: ceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: hostvars[item].cephadm_is_ceph_node

- name: Add host to nonceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: nonceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: not hostvars[item].cephadm_is_ceph_node

- name: Set first_node and bootstrap condition
  set_fact:
    cephadm_first_node: >-
      {{ (groups['ceph_nodes'][0] if (groups['ceph_nodes'] is defined and groups['ceph_nodes'] | length > 0)
         else groups['cluster_machines'][0]) | trim }}
    cephadm_do_bootstrap: "{{ (groups['ceph_nodes'] | default([])) | length == 0 }}"
  run_once: true

- name: Compute list of nodes to add as monitors
  set_fact:
    cephadm_mon_nodes_to_add: "{{ (groups['nonceph_nodes'] | default([]) | difference([cephadm_first_node])) if cephadm_do_bootstrap else groups['nonceph_nodes'] | default([]) }}"
  run_once: true

- name: Debug situation
  debug:
    msg: |
      cephadm_first_node = {{ cephadm_first_node }}
      cephadm_do_bootstrap = {{ cephadm_do_bootstrap }}
      cephadm_mon_nodes_to_add = {{ cephadm_mon_nodes_to_add | default([]) }}
      ceph_nodes = {{ groups['ceph_nodes'] | default([]) }}
      nonceph_nodes = {{ groups['nonceph_nodes'] | default([]) }}
  run_once: true

# === Registry login ===
- name: Login to container registry
  ceph.cephadm.cephadm_registry_login:
    registry_url: "{{ cephadm_registry_url }}"
    registry_username: "{{ cephadm_registry_username }}"
    registry_password: "{{ cephadm_registry_password }}"
  when:
    - cephadm_registry_url | default('') != ''
    - cephadm_registry_username | default('') != ''
    - cephadm_registry_password | default('') != ''
  no_log: true

# === Bootstrap if currently no ceph nodes ===
- name: Upload file ceph.conf needed for bootstrapping
  template:
      src: ceph.conf.j2
      dest: /tmp/ceph.conf
  run_once: true
  delegate_to: "{{ cephadm_first_node }}"
  when: cephadm_do_bootstrap | bool

- name: Bootstrap Ceph cluster
  ceph.cephadm.cephadm_bootstrap:
    mon_ip: "{{ hostvars[cephadm_first_node]['cluster_ip_addr'] }}"
    image: "{{ cephadm_image }}"
    skip_dashboard: true
    skip_monitoring_stack: true
    skip_firewalld: true
    config: /tmp/ceph.conf
    ssh_user: cephadm
    cluster_network: "{{ cluster_network | default(omit) }}"
    allow_overwrite: false
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true
  when: cephadm_do_bootstrap | bool

# === Apply ceph_conf_overrides ===
- name: Apply ceph_conf_overrides
  include_tasks: apply_ceph_conf_section.yml
  loop: "{{ ceph_conf_overrides | dict2items }}"
  loop_control:
    loop_var: ceph_conf_section
    label: "{{ ceph_conf_section.key }}"
  when: ceph_conf_overrides is defined

# === SSH key distribution ===
- name: Get ceph public key from cluster
  command: cephadm shell -- ceph cephadm get-pub-key
  register: cephadm_pub_key_result
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true
  changed_when: false
  when: cephadm_mon_nodes_to_add | length > 0

- name: Set ceph public key fact
  set_fact:
    cephadm_ceph_pubkey: "{{ cephadm_pub_key_result.stdout }}"
  run_once: true
  when: cephadm_mon_nodes_to_add | length > 0

- name: Add the ceph pubkey to each target node
  ansible.posix.authorized_key:
    user: cephadm
    state: present
    key: "{{ cephadm_ceph_pubkey }}"
  with_items: "{{ cephadm_mon_nodes_to_add }}"
  loop_control:
    loop_var: target_node
  delegate_to: "{{ target_node }}"
  run_once: true
  when: cephadm_mon_nodes_to_add | length > 0

# === Add hosts to Ceph orchestrator ===
- name: Add hosts to Ceph orchestrator
  ceph.cephadm.ceph_orch_host:
    name: "{{ hostvars[item]['hostname'] }}"
    address: "{{ hostvars[item]['cluster_ip_addr'] }}"
    set_admin_label: true
    state: present
  loop: "{{ cephadm_mon_nodes_to_add }}"
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true
  when: cephadm_mon_nodes_to_add | length > 0

- name: Confirm monitors are in monmap
  command: ceph mon dump
  register: cephadm_mon_dump
  retries: 30
  delay: 5
  changed_when: false
  run_once: true
  delegate_to: "{{ cephadm_first_node }}"
  until: "hostvars[item]['hostname'] in cephadm_mon_dump.stdout"
  loop: "{{ cephadm_mon_nodes_to_add }}"
  when: cephadm_mon_nodes_to_add | length > 0

# === OSDs ===
- name: Get list of current OSD daemons and their hosts
  command: ceph orch ps --daemon-type=osd --format json
  register: cephadm_osd_ps
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true
  changed_when: false

- name: Set fact for existing OSD hosts
  set_fact:
    cephadm_existing_osd_hosts: "{{ cephadm_osd_ps.stdout | from_json | map(attribute='hostname') | list | unique }}"
  run_once: true

- name: Debug cephadm_existing_osd_hosts
  debug:
    msg: "Nodes with existing OSDs: {{ cephadm_existing_osd_hosts }}"
  run_once: true

- name: Set list of nodes that need OSDs
  set_fact:
    cephadm_nodes_needing_osds: "{{ groups['cluster_machines'] | map('extract', hostvars, 'hostname') | difference(cephadm_existing_osd_hosts) }}"
  run_once: true

- name: Debug nodes needing OSDs
  debug:
    msg: "Nodes that need OSDs: {{ cephadm_nodes_needing_osds }}"
  run_once: true

- name: Find cephadm location
  command: which cephadm
  register: cephadm_path
  failed_when: false
  changed_when: false
  environment:
    PATH: "{{ ansible_env.PATH }}:/usr/local/bin"

- name: Set cephadm binary path
  set_fact:
    cephadm_bin: "{{ cephadm_path.stdout if cephadm_path.rc == 0 else '/usr/local/bin/cephadm' }}"

- name: Zap the volume on nodes that need OSDs
  command: "{{ cephadm_bin }} --image {{ cephadm_image }} ceph-volume lvm zap vg_ceph/lv_ceph"
  delegate_to: "{{ item }}"
  run_once: true
  when: hostvars[item]['hostname'] in cephadm_nodes_needing_osds
  loop: "{{ groups['cluster_machines'] }}"
  changed_when: true

# === Apply service specs ===
- name: Apply service specs
  ceph.cephadm.ceph_orch_apply:
    spec: "{{ lookup('template', item) }}"
  loop:
    - spec_crash.yaml.j2
    - spec_mgr.yaml.j2
    - spec_mon.yaml.j2
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true

- name: Apply OSD service specs
  ceph.cephadm.ceph_orch_apply:
    spec: "{{ lookup('template', 'spec_osd.yaml.j2') }}"
  vars:
    osd_host: "{{ item }}"
    osd_index: "{{ idx + 1 }}"
  loop: "{{ groups['osds'] }}"
  loop_control:
    index_var: idx
  delegate_to: "{{ cephadm_first_node }}"
  run_once: true
  when: hostvars[item]['hostname'] in cephadm_nodes_needing_osds

# === Health check ===
- name: Confirm cluster is ok
  command: ceph status --format=json
  register: cephadm_cephs
  retries: 30
  delay: 10
  changed_when: false
  run_once: true
  delegate_to: "{{ cephadm_first_node }}"
  until: cephadm_cephs.stdout | from_json | community.general.json_query('health.status') == "HEALTH_OK"

# === RBD pool and CephX user (SEAPATH-specific) ===
- name: Check if RBD pool exists
  shell:
    cmd: set -o pipefail && ceph osd lspools | grep -w rbd
    executable: /bin/bash
  register: cephadm_rbd_pool_check
  changed_when: false
  failed_when: false

- name: Create RBD pool if it doesn't exist
  command: ceph osd pool create rbd
  when: cephadm_rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ cephadm_first_node }}"

- name: Enable RBD application on the pool
  command: ceph osd pool application enable rbd rbd
  when: cephadm_rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ cephadm_first_node }}"

- name: Check if CephX user client.libvirt exists
  command: ceph auth get client.libvirt
  register: cephadm_cephx_user_check
  changed_when: false
  failed_when: false

- name: Create CephX user client.libvirt if it does not exist
  command: >
    ceph auth add client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephadm_cephx_user_check.rc != 0
  changed_when: true

- name: Update CephX user client.libvirt permissions if it exists
  command: >
    ceph auth caps client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephadm_cephx_user_check.rc == 0
  changed_when: true
