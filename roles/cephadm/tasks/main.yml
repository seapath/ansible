# Copyright (C) 2025 RTE
# SPDX-License-Identifier: Apache-2.0

---
- include_vars: "{{ seapath_distro }}.yml"

- name: Ensure group "cephadm" exists
  group:
    name: cephadm
    gid: 1001
    state: present

- name: Ensure user "cephadm" exists
  user:
    name: cephadm
    uid: 1001
    group: cephadm
    create_home: yes

- name: Ensure group "containerized-ceph" exists
  group:
    name: containerized-ceph
    gid: 167
    state: present
  when: seapath_distro == "Debian"

- name: Ensure user "containerized-ceph" exists with nologin (already exists on centos/oraclelinux)
  user:
    name: containerized-ceph
    uid: 167
    group: containerized-ceph
    create_home: no
    shell: /sbin/nologin
  when: seapath_distro == "Debian"

- name: Set cephadm user sudo permissions
  copy:
    src: cephadm_sudoers
    dest: /etc/sudoers.d/cephadm

- name: Download cephadm
  get_url:
    url: "https://download.ceph.com/rpm-{{ cephadm_release }}/el9/noarch/cephadm"
    dest: "/tmp/cephadm"
    mode: '0755'
  when: cephadm_downloadbinary is true

- name: Copy cephadm to /usr/local/bin
  copy:
    src: "/tmp/cephadm"
    dest: "/usr/local/bin/cephadm"
    mode: '0755'
    remote_src: yes
  when: cephadm_installbinary is true

- name: Install cephadm repository
  command: "{{ '/tmp/cephadm' if not cephadm_installbinary else 'cephadm' }} add-repo --release {{ cephadm_release_name }}"
  changed_when: true
  when: cephadm_installrepo is true

- name: Install cephadm
  command: /tmp/cephadm install
  changed_when: true
  when: cephadm_installpackage is true

- name: Install ceph-common package
  command: cephadm install ceph-common
  changed_when: true
  when: cephadm_installcommon is true

#- name: Disable ceph-crash non-containerized service
#  ansible.builtin.systemd:
#    name: ceph-crash.service
#    enabled: false
#    state: stopped

#- name: Create /usr/local/bin/ceph with 755 permissions
#  copy:
#    dest: /usr/local/bin/ceph
#    content: |
#      #!/bin/bash
#      if [ $# -eq 0 ]; then
#        exec cephadm shell
#      else
#        exec cephadm shell -- ceph "$@"
#      fi
#    mode: '0755'
#    owner: root
#    group: root

- name: Check if host is part of a Ceph cluster
  command: "ceph -s"
  register: ceph_status
  ignore_errors: true
  changed_when: false

- name: Mark host as ceph or nonceph
  set_fact:
    is_ceph_node: "{{ ceph_status.rc == 0 }}"

- name: Add host to ceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: ceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: hostvars[item].is_ceph_node

- name: Add host to nonceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: nonceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: not hostvars[item].is_ceph_node

- name: Set first_node and bootstrap condition
  set_fact:
    first_node: >-
      {{ (groups['ceph_nodes'][0] if (groups['ceph_nodes'] is defined and groups['ceph_nodes'] | length > 0)
         else groups['cluster_machines'][0]) | trim }}
    do_bootstrap: "{{ (groups['ceph_nodes'] | default([])) | length == 0 }}"
  run_once: true

- name: Compute list of nodes to add as monitors
  set_fact:
    mon_nodes_to_add: "{{ (groups['nonceph_nodes'] | default([]) | difference([first_node])) if do_bootstrap else groups['nonceph_nodes'] | default([]) }}"
  run_once: true

- name: Debug situation
  debug:
    msg: |
      first_node = {{ first_node }}
      do_bootstrap = {{ do_bootstrap }}
      mon_nodes_to_add = {{ mon_nodes_to_add | default([]) }}
      ceph_nodes = {{ groups['ceph_nodes'] | default([]) }}
      nonceph_nodes = {{ groups['nonceph_nodes'] | default([]) }}
  run_once: true

- name: Ensure insecure registry localhost:5000 is in /etc/containers/registries.conf
  ansible.builtin.blockinfile:
    path: /etc/containers/registries.conf
    marker: "# {mark} ANSIBLE MANAGED BLOCK for insecure registries"
    block: |
        [[registry]]
        insecure = true
        location = "localhost"

- name: Ensure /var/lib/registry exists
  ansible.builtin.file:
    path: /var/lib/registry
    state: directory
    owner: root
    group: root
    mode: '0755'

- name: Pull needed Docker images
  containers.podman.podman_image:
    name: "{{ item }}"
  when: cephadm_pullimages
  loop:
    - "docker.io/library/registry:2"
    - "quay.io/ceph/ceph:v{{ cephadm_release }}"

- name: Run local container registry using Podman
  containers.podman.podman_container:
    name: registry
    image: registry:2
    state: started
    detach: true
    privileged: true
    ports:
      - "5000:5000"
    volume:
      - /var/lib/registry:/var/lib/registry
    restart_policy: always

- name: Tag ceph image for local registry
  command: >
    podman tag quay.io/ceph/ceph:v{{ cephadm_release }} localhost:5000/ceph:v{{ cephadm_release }}
  changed_when: true

- name: Push ceph image to local registry
  command: >
    podman push localhost:5000/ceph:v{{ cephadm_release }}
  changed_when: true

# === Bootstrap if currently no ceph nodes ===
- name: Upload file ceph.conf needed for bootstrapping
  template:
      src: ceph.conf.j2
      dest: /tmp/ceph.conf
  run_once: true
  delegate_to: "{{ first_node }}"
  when: do_bootstrap | bool

- name: Show bootstrap command
  ansible.builtin.debug:
    msg: >
      cephadm
      --image localhost:5000/ceph:v{{ cephadm_release }}
      bootstrap
      --skip-monitoring-stack
      --skip-dashboard
      --skip-firewalld
      --config /tmp/ceph.conf
      --ssh-user cephadm
      --mon-ip {{ hostvars[first_node]['cluster_ip_addr'] }}

- name: Bootstrap Ceph cluster
  command: >
      cephadm
      --image localhost:5000/ceph:v{{ cephadm_release }}
      bootstrap
      --skip-monitoring-stack
      --skip-dashboard
      --skip-firewalld
      --config /tmp/ceph.conf
      --ssh-user cephadm
      --mon-ip {{ hostvars[first_node]['cluster_ip_addr'] }}
  changed_when: true
  failed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  register: ceph_bootstrap_result
  when: do_bootstrap | bool

- name: Show bootstrap result
  ansible.builtin.debug:
    var: ceph_bootstrap_result

# === adding cephadm key on other nodes ===
- name: Check if /etc/ceph/ceph.pub exists on first_node
  stat:
    path: /etc/ceph/ceph.pub
  register: ceph_pub_stat
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mon_nodes_to_add | length > 0

- name: Get cephadm user entry
  getent:
    database: passwd
    key: cephadm
  delegate_to: "{{ first_node }}"
  run_once: true

- name: Fetch the ceph keyfile if it exists, otherwise fetch authorized_keys
  fetch:
    src: >-
      {{
        ceph_pub_stat.stat.exists
        | ternary(
            '/etc/ceph/ceph.pub',
            getent_passwd['cephadm'][4] ~ '/.ssh/authorized_keys'
          )
      }}
    dest: "/tmp/ceph.pub"
    flat: true
  delegate_to: "{{ first_node }}"
  run_once: true
  when: mon_nodes_to_add | length > 0

- name: Read the key from the local file
  set_fact:
    ceph_pubkey: "{{ lookup('file', '/tmp/ceph.pub') }}"
  delegate_to: localhost
  run_once: true
  when: mon_nodes_to_add | length > 0

- name: Add the ceph pubkey to each target node
  ansible.posix.authorized_key:
    user: cephadm
    state: present
    key: "{{ ceph_pubkey }}"
  with_items: "{{ mon_nodes_to_add }}"
  loop_control:
    loop_var: target_node
  delegate_to: "{{ target_node }}"
  run_once: true
  when: mon_nodes_to_add | length > 0

# === adding monitor on other nodes ===
- name: Add hosts to Ceph orchestrator with _admin label
  command: "ceph orch host add {{ hostvars[item]['hostname'] }} --labels _admin"
  loop: "{{ mon_nodes_to_add }}"
  changed_when: true
  failed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mon_nodes_to_add | length > 0

- name: Confirm monitors are in monmap
  command: ceph mon dump
  register: mon_dump
  retries: 30
  delay: 5
  changed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  until: "hostvars[item]['hostname'] in mon_dump.stdout"
  loop: "{{ mon_nodes_to_add }}"
  when: mon_nodes_to_add | length > 0

# === OSDs now ===
- name: Get list of current OSD daemons and their hosts
  command: ceph orch ps --daemon-type=osd --format json
  register: osd_ps
  delegate_to: "{{ first_node }}"
  run_once: true
  changed_when: false
- name: Set fact for existing OSD hosts
  set_fact:
    existing_osd_hosts: "{{ osd_ps.stdout | from_json | map(attribute='hostname') | list | unique }}"
  run_once: true
- name: Debug existing_osd_hosts
  debug:
    msg: "Nodes with existing OSDs: {{ existing_osd_hosts }}"
  run_once: true
- name: Set list of nodes that need OSDs
  set_fact:
    nodes_needing_osds: "{{ groups['cluster_machines'] | map('extract', hostvars, 'hostname') | difference(existing_osd_hosts) }}"
  run_once: true
- name: Debug nodes needing OSDs
  debug:
    msg: "Nodes that need OSDs: {{ nodes_needing_osds }}"
  run_once: true

- name: Zap the volume on nodes that need OSDs
  command: "cephadm --image localhost:5000/ceph:v{{ cephadm_release }} ceph-volume lvm zap vg_ceph/lv_ceph"
  delegate_to: "{{ item }}"
  run_once: true
  when: hostvars[item]['hostname'] in nodes_needing_osds
  loop: "{{ groups['cluster_machines'] }}"
  changed_when: true

- name: Copy ceph orch spec file
  template:
    src: "{{ cephadm_spec_path | default('spec.yaml.j2') }}"
    dest: /tmp/t.yaml
    mode: 0644
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Add OSD daemon on nodes that need OSDs
  command: cephadm --image localhost:5000/ceph:v{{ cephadm_release }} shell -v /tmp/t.yaml:/tmp/t.yaml:ro -- ceph orch apply -i /tmp/t.yaml
  delegate_to: "{{ first_node }}"
  run_once: true
  changed_when: true
  failed_when: false

- name: Confirm cluster is ok
  command: ceph status --format=json
  register: cephs
  retries: 30
  delay: 10
  changed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  until: cephs.stdout | from_json | community.general.json_query('health.status') == "HEALTH_OK"

- name: Check if RBD pool exists
  shell:
    cmd: set -o pipefail && ceph osd lspools | grep -w rbd
    executable: /bin/bash
  register: rbd_pool_check
  changed_when: false
  failed_when: false

- name: Create RBD pool if it doesn't exist
  command: ceph osd pool create rbd
  when: rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Enable RBD application on the pool
  command: ceph osd pool application enable rbd rbd
  when: rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Check if CephX user client.libvirt exists
  command: ceph auth get client.libvirt
  register: cephx_user_check
  changed_when: false
  failed_when: false

- name: Create CephX user client.libvirt if it does not exist
  command: >
    ceph auth add client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephx_user_check.rc != 0
  changed_when: true

- name: Update CephX user client.libvirt permissions if it exists
  command: >
    ceph auth caps client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephx_user_check.rc == 0
  changed_when: true

#- name: Update logrotate configuration for Ceph
#  replace:
#    path: "{{ cephadm_logrorateceph_path }}"
#    regexp: '^/var/log/ceph/\*.log'
#    replace: '/var/log/ceph/!(cephadm).log'

- name: Stop and remove Podman registry container
  containers.podman.podman_container:
    name: registry
    state: absent
