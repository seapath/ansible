# This inventory describes a SEAPATH cluster. It contains all required variables for a default deployment
# Replace all the TODOs to fit your physical machines.
# More informations on SEAPATH Wiki: https://lf-energy.atlassian.net/wiki/x/BIDtGQ

# Note : This inventory uses a three hypervisors setup.
# If you want to switch to a two hypervisor + one observer setup, you must :
# - choose which machine will be the observer (ex: node3)
# - remove the machine from the hypervisors group
# - add the machine to the observers group
# - remove the machine from the osds group
# - optionally, remove the ptp_interface variable of the machine

---
all:
  hosts:
    node1:
      ansible_host: 192.168.200.121 # Administration IP. TODO
      network_interface: eno1 # Administration interface. TODO
      ptp_interface: "eno12419" # PTP interface. TODO

    node2:
      ansible_host: 192.168.200.122 # Administration IP. TODO
      network_interface: eno1 # Administration interface. TODO
      ptp_interface: "eno12419" # PTP interface. TODO

    node3:
      ansible_host: 192.168.200.123 # Administration IP. TODO
      network_interface: eno1 # Administration interface. TODO
      ptp_interface: "eno12419" # PTP interface. TODO

  vars:
    # Debian specific, remove if you use Yocto. TODO
    admin_user: admin

    # Main network configuration
    gateway_addr: "192.168.200.1" # Administration Gateway. TODO
    dns_servers: "192.168.200.1" # DNS servers. Remove if not used. TODO
    subnet: 24 # Subnet mask in CIDR notation. TODO
    ntp_servers: # NTP servers IP. TODO
      - "185.254.101.25" # public NTP server example

    # SEAPATH ansible variables
    ansible_connection: ssh
    ansible_python_interpreter: /usr/bin/python3
    ansible_remote_tmp: /tmp/.ansible/tmp
    ansible_user: ansible
    ip_addr: "{{ ansible_host }}"
    hostname: "{{ inventory_hostname }}"
    apply_network_config: true

# Three hypervisors setup. All machines are part of the hypervisor group.
hypervisors:
  hosts:
    node1:
    node2:
    node3:
  vars:
    isolcpus: "4-N" # The list of CPUs to isolate. TODO
    livemigration_user: libvirtadmin

# Three hypervisors setup. The observer group is empty.
observers:
  hosts:

# Cluster part. More information on the wiki: https://lf-energy.atlassian.net/wiki/x/SgB9GQ
cluster_machines:
  hosts:
    node1:
      team0_0: "eno2" # Cluster interface connected to node2. TODO
      team0_1: "eno3" # Cluster interface connected to node3. TODO
      cluster_ip_addr: "192.168.55.1" # IP address of this node on the cluster. TODO
      cluster_next_ip_addr : "192.168.55.2" # IP address of node2 on the cluster. TODO
      cluster_previous_ip_addr : "192.168.55.3" # IP address of node3 on the cluster. TODO

    node2:
      team0_0: "eno2" # Cluster interface connected to node3. TODO
      team0_1: "eno3" # Cluster interface connected to node1. TODO
      cluster_ip_addr: "192.168.55.2" # IP address of this node on the cluster. TODO
      cluster_next_ip_addr : "192.168.55.3" # IP address of node3 on the cluster. TODO
      cluster_previous_ip_addr :  "192.168.55.1" # IP address of node1 on the cluster. TODO

    node3:
      team0_0: "enp2s0" # Cluster interface connected to node1. TODO
      team0_1: "enp3s0" # Cluster interface connected to node2. TODO
      cluster_ip_addr: "192.168.55.3" # IP address of this node on the cluster. TODO
      cluster_next_ip_addr : "192.168.55.1" # IP address of node1 on the cluster. TODO
      cluster_previous_ip_addr :  "192.168.55.2" # IP address of node2 on the cluster. TODO
      br_rstp_priority: 12288 # Do not modify, used by RSTP

# Ceph monitor. All machines in the cluster must be part of mons groups
# Note : By default SEAPATH uses ceph-ansible to configure Ceph.
# You can also switch to cephadm deployment by setting force_cephadm to true (Only available on Debian for now)
mons:
  hosts:
    node1:
    node2:
    node3:
  vars:
    ceph_origin: distro
    cluster_network: "192.168.55.0/24" # IP range of your cluster. TODO
    public_network: "{{ cluster_network }}"
    monitor_address: "{{ cluster_ip_addr }}"
    configure_firewall: false
    ntp_service_enabled: false
    dashboard_enabled: false
    ceph_conf_overrides:
      global:
        osd_pool_default_size: "{{ groups['hypervisors'] | length }}"
        osd_pool_default_min_size: 1
        osd_pool_default_pg_num: 128
        osd_pool_default_pgp_num: 128
        osd_crush_chooseleaf_type: 1
        mon_osd_min_down_reporters: 1
      mon:
        auth_allow_insecure_global_id_reclaim: false
      osd:
        osd memory target: 8076326604


# Ceph OSD. Machines that will be used as OSDs (which will store data)
osds:
  hosts:
    node1:
      # Set this to the path of the disk that will contains ceph data.
      # The path can be found in "/dev/disk/by-path/"
      ceph_osd_disk: "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:1:0" # TODO
    node2:
      ceph_osd_disk: "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:1:0" # TODO
    node3:
      ceph_osd_disk: "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:1:0" # TODO
  vars:
    devices: "{{ ceph_osd_disk }}"

# Ceph clients. All machines in the cluster must be part of clients groups
clients:
  hosts:
    node1:
    node2:
    node3:
  vars:
    user_config: true
    rbd:
      name: "rbd"
      application: "rbd"
      pg_autoscale_mode: on
      target_size_ratio: 1
    pools:
      - "{{ rbd }}"
    keys:
      - name: client.libvirt
        caps:
          mon: 'profile rbd, allow command "osd blacklist"'
          osd: "allow class-read object_prefix rbd_children, profile rbd pool=rbd"
        mode: "{{ ceph_keyring_permissions }}"

# Empty groups to prevent warnings, mostly ceph-ansible
grafana-server:
iscsigws:
iscsi-gws:
mdss:
mgrs:
nfss:
rbdmirrors:
rgwloadbalancers:
rgws:
standalone_machine:

...
