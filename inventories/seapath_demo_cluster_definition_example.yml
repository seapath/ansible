---
all:
    children:
        cluster_machines:
            children:
                hypervisors:
                    hosts:
                        # IPs need to be configured
                        hypervisor1:
                            # ansible variables
                            ansible_host: 192.168.216.135

                        hypervisor2:
                            # ansbile variables
                            ansible_host: 192.168.216.136

                        hypervisor3:
                            # ansbile variables
                            ansible_host: 192.168.216.137

                    # hypervisors common vars
                    vars:
                        # Main network interface configuration
                        network_interface: enp88s0 #main interface
                        ip_addr: "{{ ansible_host }}"

        # Ceph groups
        # All machines in the cluster must be part of mons groups
        mons:
            hosts:
                hypervisor1:
                hypervisor2:
                hypervisor3:
        # Machines that will be used as OSDs (which will store data)
        osds:
            hosts:
                hypervisor1:
                hypervisor2:
                hypervisor3:

            vars:
                # Ceph settings
                # ceph_osd_disk needs to be set to the "/dev/disk/by-path/" link of the disk used by the osd
                # Location of the disk and space allocated for Ceph need to be configured
                ceph_osd_disk: "/dev/disk/by-path/pci-0000:01:00.0-nvme-1"
                # Required variables by ceph-ansible:
                lvm_volumes: # One disk installation. Create a 100 GiB partition for Ceph.
                    - data: lv_ceph # Name of the logical volume to use for the CEPH OSD volume
                      data_vg: vg_ceph # Name of the volume group to use for the CEPH OSD volume
                      data_size: 80G # Size of the logical volume, default in megabytes, possible values: [0-9]+[bBsSkKmMgGtTpPeE]
                      device: "{{ ceph_osd_disk }}"
                      device_number: 3 # Number of the partition in the ceph_osd_disk
                      device_size: 100GiB # Size of the partition in the ceph_osd_disk, default in megabytes, possible units: B, KB, KiB, MB, MiB, GB, GiB, TB, TiB

        clients:
            hosts:
                hypervisor1:
                hypervisor2:
                hypervisor3:
            vars:
              # Required variables by ceph-ansible.
              # These are SEAPATH needed overrides. Do not change unless you know exactly what you are doing
              user_config: true
              rbd:
                name: "rbd"
                application: "rbd"
                pg_autoscale_mode: on
                target_size_ratio: 1
              pools:
                - "{{ rbd }}"
              keys:
                - name: client.libvirt
                  caps:
                    mon: 'profile rbd, allow command "osd blacklist"'
                    osd: "allow class-read object_prefix rbd_children, profile rbd pool=rbd"
                  mode: "{{ ceph_keyring_permissions }}"
    # Common vars for all hosts
    vars:
        # Ansible vars
        ansible_connection: ssh
        ansible_user: ansible
        ansible_python_interpreter: /usr/bin/python3
        ansible_ssh_common_args: '-o StrictHostKeyChecking=no'
        ansible_remote_tmp: /tmp/.ansible/tmp

        # Main network configuration
        # Need to be configured
        gateway_addr: 192.168.216.1
        dns_servers: "192.168.216.1"
        apply_network_config: false # do we restart the ovs_config script to apply the changes to the ovs topology (default: false)
        #skip_reboot_setup_network: true # if we do not apply the changes (apply_network_config=false), then the network playbook will reboot the servers at the end. You can choose to skip this reboot here (default: false). It is recommended to let it to false except if you fully understand the network configuration process.
        skip_recreate_team0_config: true # one network configuration
        remove_all_networkd_config: true  # if defined and true, the network playbook will start by wiping the /etc/systemd/network/ directory content, this can help cleaning old conflicting files.

        # Need to be configured
        ntp_servers:
            - "192.168.216.1"

        # Ceph settings
        # Required variables by ceph-ansible.
        # These are SEAPATH needed overrides. Do not change unless specified (osd_pool_default*) or you know exactly what you are doing
        configure_firewall: false
        ntp_service_enabled: true
        ceph_origin: distro
        monitor_address: "{{ ansible_host }}"
        public_network: "192.168.216.0/24" # Need to be configured
        cluster_network: "{{ public_network }}"
        ceph_conf_overrides:
          global:
            osd_pool_default_size: 3 # to be set to the number storage nodes
            osd_pool_default_min_size: 2 # to be set to the number of nodes - the number of nodes we are allowing ourselve to lose = the minimum number of nodes needed online for the pool to be available
            osd_pool_default_pg_num: 128
            osd_pool_default_pgp_num: 128
            osd_crush_chooseleaf_type: 1
            mon_osd_min_down_reporters: 1
          mon:
            auth_allow_insecure_global_id_reclaim: false
          osd:
            osd_min_pg_log_entries: 500
            osd_max_pg_log_entries: 500
            osd memory target: 8076326604
        dashboard_enabled: false

        # Grub password
        # The password hash generated with "grub-mkpasswd-pbkdf2 -c 65536 -s 256 -l 64", in this example the pass is "toto"
        grub_password: grub.pbkdf2.sha512.65536.E291D66AEEB3C22BD6B019C5C3587A3094AE93D61E20D134EC6324925AE5045DCA61EF30B3BD04B4D6F7360B9C9B242AA68B1643CCB269C53658EC959B5964ADB9C9D5FAA280A291D8F95E3F255254A4119A2431AFE797F1949EE4FBBC4C74281C550C83DAED26C254224061BEFCEEBF8091A8D1BE406EBB3A3E8A519E36B4FE161BE191D407193E5DFEBCC09F8822B4060EA9CD1E6B8677D40D32826EF025CA494BCD209032F7CF2A4A2E74717E6D17E87A62AAA93E458C96F983E69FBFC4FD602403988EAF5AADCA4B5B145B0F6C6FFB53F55CD6C56C15C17B2F8A5B3A214F3140470566597760D9388084AE978DB5C0EBF7C868A855DB38DACA47A010417A.FDA0C3188FAC87FFA04D862AC9B1020F29FEEA3B9590BE534330D8C7CAB71444CBF527E39D7DAE640545139202B9CA77822CCE21AB1134110F6AF3EFD793E848


        # Extra kernel modules to be loaded at boot (you may need this depending on your hardware)
        extra_kernel_modules: []

        apt_repo:
          - http://ftp.fr.debian.org/debian bookworm main contrib non-free non-free-firmware
          - http://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware
          - http://ftp.fr.debian.org/debian bookworm-backports main contrib non-free non-free-firmware
          - https://artifacts.elastic.co/packages/8.x/apt stable main

        # Default user with admin privileges, and password hash
        # Need to be configured
        admin_user: virtu
        admin_passwd: "XXXXXX"
        # SSH public keys for the admin_user
        # Need to be configured
        admin_ssh_keys:
          - "ssh-rsa key1XXX"
          - "ssh-rsa key2XXX"
        # account used for libvirt live-migration
        livemigration_user: livemigration
        interface_to_wait_for: br0
