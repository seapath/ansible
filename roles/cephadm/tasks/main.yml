# Copyright (C) 2025 RTE
# SPDX-License-Identifier: Apache-2.0

---
- include_vars: "{{ seapath_distro }}.yml"

- name: Install cephadm binaries
  when: cephadm_install is true
  block:
    - name: Download cephadm
      get_url:
        url: "https://download.ceph.com/rpm-{{ cephadm_release }}/el9/noarch/cephadm"
        dest: "/usr/local/bin/cephadm"
        mode: '0755'
#    - name: Add Ceph repository
#      command: /usr/local/bin/cephadm add-repo --release squid
#      changed_when: true
#    - name: Install ceph-common package
#      command: /usr/local/bin/cephadm install ceph-common ceph-volume
#      changed_when: true

#- name: Disable ceph-crash non-containerized service
#  ansible.builtin.systemd:
#    name: ceph-crash.service
#    enabled: false
#    state: stopped

- name: Create /usr/local/bin/ceph with 755 permissions
  copy:
    dest: /usr/local/bin/ceph
    content: |
      #!/bin/bash
      if [ $# -eq 0 ]; then
        exec /usr/local/bin/cephadm shell
      else
        exec /usr/local/bin/cephadm shell -- ceph "$@"
      fi
    mode: '0755'
    owner: root
    group: root

- name: Ensure group "cephadm" exists
  group:
    name: cephadm
    gid: 1001
    state: present

- name: Ensure user "cephadm" exists
  user:
    name: cephadm
    uid: 1001
    group: cephadm
    create_home: yes

- name: Ensure group "containerized-ceph" exists
  group:
    name: containerized-ceph
    gid: 167
    state: present
  when: seapath_distro == "Debian"

- name: Ensure user "containerized-ceph" exists with nologin (already exists on centos/oraclelinux)
  user:
    name: containerized-ceph
    uid: 167
    group: containerized-ceph
    create_home: no
    shell: /sbin/nologin
  when: seapath_distro == "Debian"

- name: Set cephadm user sudo permissions
  copy:
    src: cephadm_sudoers
    dest: /etc/sudoers.d/cephadm


- name: Check if host is part of a Ceph cluster
  command: "/usr/local/bin/ceph -s"
  register: ceph_status
  ignore_errors: true
  changed_when: false

- name: Mark host as ceph or nonceph
  set_fact:
    is_ceph_node: "{{ ceph_status.rc == 0 }}"

- name: Add host to ceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: ceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: hostvars[item].is_ceph_node

- name: Add host to nonceph_nodes group
  add_host:
    name: "{{ item }}"
    groups: nonceph_nodes
  run_once: true
  loop: "{{ groups['cluster_machines'] }}"
  when: not hostvars[item].is_ceph_node

- name: Set first_node and bootstrap condition
  set_fact:
    first_node: >-
      {{ (groups['ceph_nodes'][0] if (groups['ceph_nodes'] is defined and groups['ceph_nodes'] | length > 0)
         else groups['cluster_machines'][0]) | trim }}
    do_bootstrap: "{{ (groups['ceph_nodes'] | default([])) | length == 0 }}"
  run_once: true

- name: Compute list of nodes to add as monitors
  set_fact:
    mon_nodes_to_add: "{{ (groups['nonceph_nodes'] | default([]) | difference([first_node])) if do_bootstrap else groups['nonceph_nodes'] | default([]) }}"
  run_once: true

- name: Debug situation
  debug:
    msg: |
      first_node = {{ first_node }}
      do_bootstrap = {{ do_bootstrap }}
      mon_nodes_to_add = {{ mon_nodes_to_add | default([]) }}
      ceph_nodes = {{ groups['ceph_nodes'] | default([]) }}
      nonceph_nodes = {{ groups['nonceph_nodes'] | default([]) }}
  run_once: true

# === Bootstrap if currently no ceph nodes ===
- name: Upload file ceph.conf needed for bootstrapping
  template:
      src: ceph.conf.j2
      dest: /tmp/ceph.conf
  run_once: true
  delegate_to: "{{ first_node }}"
  when: do_bootstrap | bool

- name: Bootstrap Ceph cluster
  command: >
      /usr/local/bin/cephadm bootstrap
      --skip-monitoring-stack
      --skip-dashboard
      --config /tmp/ceph.conf
      --ssh-user cephadm
      --mon-ip {{ hostvars[first_node]['cluster_ip_addr'] }}
  changed_when: true
  failed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  register: ceph_bootstrap_result
  when: do_bootstrap | bool

# === adding monitor on other nodes ===
- name: Check if /etc/ceph/ceph.pub exists on first_node
  stat:
    path: /etc/ceph/ceph.pub
  register: ceph_pub_stat
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mon_nodes_to_add | length > 0

- name: Fetch the ceph keyfile if it exists, otherwise fetch authorized_keys
  fetch:
    src: "{{ ceph_pub_stat.stat.exists | ternary('/etc/ceph/ceph.pub', '/home/cephadm/.ssh/authorized_keys') }}"
    dest: "/tmp/ceph.pub"
    flat: true
  delegate_to: "{{ first_node }}"
  run_once: true
  when: mon_nodes_to_add | length > 0

- name: Read the key from the local file
  set_fact:
    ceph_pubkey: "{{ lookup('file', '/tmp/ceph.pub') }}"
  delegate_to: localhost
  run_once: true
  when: mon_nodes_to_add | length > 0

- name: Add the ceph pubkey to each target node
  ansible.posix.authorized_key:
    user: cephadm
    state: present
    key: "{{ ceph_pubkey }}"
  with_items: "{{ mon_nodes_to_add }}"
  loop_control:
    loop_var: target_node
  delegate_to: "{{ target_node }}"
  run_once: true
  when: mon_nodes_to_add | length > 0

- name: Add hosts to Ceph orchestrator with _admin label
  command: "/usr/local/bin/ceph orch host add {{ hostvars[item]['hostname'] }} --labels _admin"
  loop: "{{ mon_nodes_to_add }}"
  changed_when: true
  failed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mon_nodes_to_add | length > 0

- name: Confirm monitors are in monmap
  command: /usr/local/bin/ceph mon dump
  register: mon_dump
  retries: 30
  delay: 5
  changed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  until: "hostvars[item]['hostname'] in mon_dump.stdout"
  loop: "{{ mon_nodes_to_add }}"
  when: mon_nodes_to_add | length > 0

# === Managers now ===
- name: Get current number of MGR daemons
  shell:
    cmd: set -o pipefail && /usr/local/bin/ceph orch ls --service-name=mgr | grep -o "count:.*" | cut -d":" -f2
    executable: /bin/bash
  register: mgr_count_raw
  changed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Set fact with MGR count
  set_fact:
    mgr_count: "{{ mgr_count_raw.stdout | trim }}"
  run_once: true
  delegate_to: "{{ first_node }}"

- name: "RUN /usr/local/bin/ceph orch apply mgr {{ groups['cluster_machines'] | length }}"
  command: "/usr/local/bin/ceph orch apply mgr {{ groups['cluster_machines'] | length }}"
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mgr_count | int != groups['cluster_machines'] | length
  changed_when: true

# === OSDs now ===
- name: Get list of current OSD daemons and their hosts
  command: /usr/local/bin/ceph orch ps --service-name=osd --format json
  register: osd_ps
  delegate_to: "{{ first_node }}"
  run_once: true
  changed_when: false
- name: Set fact for existing OSD hosts
  set_fact:
    existing_osd_hosts: "{{ osd_ps.stdout | from_json | map(attribute='hostname') | list | unique }}"
  run_once: true
- name: Debug existing_osd_hosts
  debug:
    msg: "Nodes with existing OSDs: {{ existing_osd_hosts }}"
  run_once: true
- name: Set list of nodes that need OSDs
  set_fact:
    nodes_needing_osds: "{{ groups['cluster_machines'] | map('extract', hostvars, 'hostname') | difference(existing_osd_hosts) }}"
  run_once: true
- name: Debug nodes needing OSDs
  debug:
    msg: "Nodes that need OSDs: {{ nodes_needing_osds }}"
  run_once: true

- name: Zap the volume on nodes that need OSDs
  command: "/usr/local/bin/cephadm ceph-volume lvm zap vg_ceph/lv_ceph"
  delegate_to: "{{ item }}"
  run_once: true
  when: hostvars[item]['hostname'] in nodes_needing_osds
  loop: "{{ groups['cluster_machines'] }}"
  changed_when: true

- name: Add OSD daemon on nodes that need OSDs
  command: "/usr/local/bin/ceph orch daemon add osd {{ hostvars[item]['hostname'] }}:/dev/vg_ceph/lv_ceph"
  delegate_to: "{{ first_node }}"
  run_once: true
  when: hostvars[item]['hostname'] in nodes_needing_osds
  loop: "{{ groups['cluster_machines'] }}"
  changed_when: true
  failed_when: false

- name: Check if RBD pool exists
  shell:
    cmd: set -o pipefail && /usr/local/bin/ceph osd lspools | grep -w rbd
    executable: /bin/bash
  register: rbd_pool_check
  changed_when: false
  failed_when: false

- name: Create RBD pool if it doesn't exist
  command: /usr/local/bin/ceph osd pool create rbd
  when: rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Enable RBD application on the pool
  command: /usr/local/bin/ceph osd pool application enable rbd rbd
  when: rbd_pool_check.rc != 0
  changed_when: true
  run_once: true
  delegate_to: "{{ first_node }}"

- name: Check if CephX user client.libvirt exists
  command: /usr/local/bin/ceph auth get client.libvirt
  register: cephx_user_check
  changed_when: false
  failed_when: false

- name: Create CephX user client.libvirt if it does not exist
  command:
    /usr/local/bin/ceph auth add client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephx_user_check.rc != 0
  changed_when: true

- name: Update CephX user client.libvirt permissions if it exists
  shell: >
    /usr/local/bin/ceph auth caps client.libvirt
    mon 'profile rbd, allow command "osd blacklist"'
    osd 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
  when: cephx_user_check.rc == 0
  changed_when: true

#- name: Update logrotate configuration for Ceph
#  replace:
#    path: "{{ cephadm_logrorateceph_path }}"
#    regexp: '^/var/log/ceph/\*.log'
#    replace: '/var/log/ceph/!(cephadm).log'
