# Copyright (C) 2022 Ansible Project
# Copyright (C) 2022, RTE (http://www.rte-france.com)
# SPDX-License-Identifier: Apache-2.0
# Import from ceph-ansible infrastructure-playbooks/shrink-mgr.yml:
# https://github.com/ceph/ceph-ansible/blob/000e93f608cd6c07ffa8d0dc76092d6234682ea8/infrastructure-playbooks/shrink-mgr.yml

---
# This playbook shrinks the Ceph monitors from your cluster
# It can remove a Ceph of monitor from the cluster and ALL ITS DATA
#
# Use it like this:
# ansible-playbook shrink-mon.yml -e mon_to_kill=ceph-mon01
#     Prompts for confirmation to shrink, defaults to no and
#     doesn't shrink the cluster. yes shrinks the cluster.
#
# ansible-playbook -e ireallymeanit=yes|no shrink-mon.yml
#     Overrides the prompt using -e option. Can be used in
#     automation scripts to avoid interactive prompt.


- name: Sanity check
  hosts: localhost
  tasks:
    - name: exit playbook, if no monitor was given
      fail:
        msg: "mon_to_kill must be declared
          Exiting shrink-cluster playbook, no monitor was removed.
           On the command line when invoking the playbook, you can use
           -e mon_to_kill=ceph-mon01 argument. You can only remove a single monitor each time the playbook runs."
      when: mon_to_kill is not defined

- name: Try to find an online Ceph machine
  hosts: "{{ mon_group_name|default('mons') }}"
  gather_facts: false
  tasks:
    - name: Wait for machine to be online
      wait_for_connection:
        timeout: 1
      ignore_errors: true
      no_log: true
      register: machine_up

- name: Register the machine to use to control Ceph
  hosts: localhost
  vars:
    mon_group_name: mons
  tasks:
    - name: Register the machine
      set_fact:
        mon_host: "{% if not hostvars[item].machine_up.failed  %}{{item}}{% else %}{{ mon_host | default(false | bool)}}{% endif %}"
        offline_host: "{% if hostvars[item].machine_up.failed  %}{{true | bool}}{% else %}{{ offline_host | default(false | bool)}}{% endif %}"
      loop: "{{ groups.get(mon_group_name) }}"
    - name: check if a Ceph machine is up
      fail:
        msg: "There is no Ceph machine available"
      when: not mon_host

- name: gather facts and check the init system
  hosts: "{{ mon_group_name|default('mons') }}"
  become: true
  tasks:
    - name: "Gather facts"
      gather_facts:
      when: not machine_up.failed


- name: Remove monitor
  hosts: "{{ hostvars['localhost'].mon_host }}"
  become: true
  vars:
    mon_group_name: mons
    offline_host: "{{ hostvars['localhost'].offline_host }}"
    mon_host: "{{ hostvars['localhost'].mon_host }}"

  pre_tasks:
    - name: exit playbook, if only one monitor is present in cluster
      fail:
        msg: "You are about to shrink the only monitor present in the cluster.
              If you really want to do that, please use the purge-cluster playbook."
      when: groups[mon_group_name] | length | int == 1

    - name: exit playbook, if the monitor is not part of the inventory
      fail:
        msg: "It seems that the host given is not part of your inventory, please make sure it is."
      when: mon_to_kill not in groups[mon_group_name]

    - import_role:
        name: ceph-defaults

    - import_role:
        name: ceph-facts
        tasks_from: container_binary

  tasks:
    - name: "set_fact container_exec_cmd build {{ container_binary }} exec command (containerized)"
      set_fact:
        container_exec_cmd: "{{ container_binary }} exec ceph-mon-{{ hostvars[mon_host]['ansible_facts']['hostname'] }}"
      when: containerized_deployment | bool

    - name: exit playbook, if can not connect to the cluster
      command: "{{ container_exec_cmd }} timeout 5 ceph --cluster {{ cluster }} health"
      register: ceph_health
      changed_when: false
      until: ceph_health.stdout.find("HEALTH") > -1
      retries: 5
      delay: 2

    - name: set_fact mon_to_kill_hostname
      set_fact:
        mon_to_kill_hostname: "{{ hostvars[mon_to_kill]['ansible_facts']['hostname'] }}"
      when: offline_host is undefined

    - name: set_fact mon_to_kill_hostname offline
      set_fact:
        mon_to_kill_hostname: "{{ mon_to_kill }}"
      when: offline_host is defined


    - name: stop monitor service(s)
      service:
        name: ceph-mon@{{ mon_to_kill_hostname }}
        state: stopped
        enabled: no
      delegate_to: "{{ mon_to_kill }}"
      failed_when: false
      when: offline_host is undefined

    - name: purge monitor store
      file:
        path: /var/lib/ceph/mon/{{ cluster }}-{{ mon_to_kill_hostname }}
        state: absent
      delegate_to: "{{ mon_to_kill }}"
      when: offline_host is undefined

    - name: remove monitor from the quorum
      command: "{{ container_exec_cmd }} ceph --cluster {{ cluster }} mon remove {{ mon_to_kill_hostname }}"
      changed_when: false
      failed_when: false

  post_tasks:
    - name: verify the monitor is out of the cluster
      command: "{{ container_exec_cmd }} ceph --cluster {{ cluster }} quorum_status -f json"
      changed_when: false
      failed_when: false
      register: result
      until: mon_to_kill_hostname not in (result.stdout | from_json)['quorum_names']
      retries: 2
      delay: 10

    - name: please remove the monitor from your ceph configuration file
      debug:
          msg: "The monitor has been successfully removed from the cluster.
          Please remove the monitor entry from the rest of your ceph configuration files, cluster wide."
      run_once: true
      when: mon_to_kill_hostname not in (result.stdout | from_json)['quorum_names']

    - name: fail if monitor is still part of the cluster
      fail:
          msg: "Monitor appears to still be part of the cluster, please check what happened."
      run_once: true
      when: mon_to_kill_hostname in (result.stdout | from_json)['quorum_names']

    - name: show ceph health
      command: "{{ container_exec_cmd }} ceph --cluster {{ cluster }} -s"
      changed_when: false

    - name: show ceph mon status
      command: "{{ container_exec_cmd }} ceph --cluster {{ cluster }} mon stat"
      changed_when: false
