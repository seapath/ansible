# Copyright (C) 2025 Red Hat, Inc.
# SPDX-License-Identifier: Apache-2.0



# --- Ceph tests ---
cukinia_log "$(_colorize yellow "--- check Ceph status ---")"

_ceph_status=$(timeout 10s ceph status 2>/dev/null)
# If cluster is not set up properly, ceph commands will never return anything.
# A timeout is necessary to catch a cluster failure

function get_ceph_status_field() {
    local field="$1"

    if test -z "${_ceph_status}"; then
        return 254
    fi
    grep "${field}:" <<< "${_ceph_status}" | cut -d":" -f2- | xargs
}

get_ceph_status_field health | grep -Pq "HEALTH_(OK|WARN)"
id "SEAPATH-00051" as "health is not error" cukinia_test $? -eq 0

get_ceph_status_field mon | grep -q "3 daemons"
id "SEAPATH-00052" as "3 monitors are configured" cukinia_test $? -eq 0

quorum_out="$(get_ceph_status_field 'out of quorum' || echo fail)"
id "SEAPATH-00053" as "3 monitors are up" cukinia_test -z "${quorum_out}"

get_ceph_status_field osd | \
    grep -qE "([2-9][0-9]*|1[0-9]+) osds: ([2-9][0-9]*|1[0-9]+) up"
id "SEAPATH-00054" as "at least 2 osds are configured and up" cukinia_test $? -eq 0

get_ceph_status_field mgr | grep -q "active"
id "SEAPATH-00055" as "a manager is active" cukinia_test $? -eq 0

# --- corosync tests ---
cukinia_log "$(_colorize yellow "--- check corosync service ---")"


id "SEAPATH-00119" as "corosync service is running" cukinia_cmd systemctl is-active corosync.service

# --- pacemaker tests ---
cukinia_log "$(_colorize yellow "--- check Pacemaker status ---")"

id "SEAPATH-00122" as "pacemaker service is running" cukinia_cmd systemctl is-active pacemaker.service


crm_output="$(crm status 2>/dev/null)"
crm_retval=$?

[ ${crm_retval} -eq 0 ] && ! grep -q "OFFLINE:" <<< "${crm_output}"
echo "${crm_output}" | id "SEAPATH-00063" as "no OFFLINE node" cukinia_test $? -eq 0

[ ${crm_retval} -eq 0 ] && grep -q "3 nodes configured" <<< "${crm_output}"
echo "${crm_output}" | id "SEAPATH-00064" as "3 nodes are configured" cukinia_test $? -eq 0


# --- libvirt tests ---
cukinia_log "$(_colorize yellow "--- Test vm_manager module libvirt part ---")"

clean_vm()
{
    if virsh list |grep -q 'test0' ; then
        virsh undefine "test0"
    fi
    rm -f /tmp/test_config.xml
}

clean_vm

id "SEAPATH-00065" as "list secrets" cukinia_test `/usr/local/bin/libvirt_cmd secrets \
    |grep -c client.libvirt` -gt 0

id "SEAPATH-00066" as "define VM from a valid configuration" cukinia_cmd \
    /usr/local/bin/libvirt_cmd define /usr/share/testdata/vm.xml


/usr/local/bin/libvirt_cmd \
    define \
    /usr/share/testdata/wrong_vm_config.xml 1>/dev/null 2>&1
id "SEAPATH-00067" as "define VM from a valid configuration" \
cukinia_test $? -ne 0

id "SEAPATH-00068" as "list VM" cukinia_test `/usr/local/bin/libvirt_cmd list \
    |grep -c "test0"` -gt 0

id "SEAPATH-00069" as "export VM configuration" cukinia_cmd /usr/local/bin/libvirt_cmd \
    export "test0" /tmp/test_config.xml

id "SEAPATH-00070" as "VM configuration has been export" cukinia_test -f /tmp/test_config.xml

clean_vm

# --- vm manager pacemaker tests ---
cukinia_log "$(_colorize yellow "--- Test vm_manager module Pacemaker part ---")"

# locate vm_manager directory
VM_MANAGER_DIR=$(/usr/bin/python3 -c 'import vm_manager as m; print(m.__path__[0])')

id "SEAPATH-00071" as "Test add VM" cukinia_cmd python3 $VM_MANAGER_DIR/helpers/tests/pacemaker/add_vm.py

id "SEAPATH-00072" as "Test stop VM" cukinia_cmd python3 $VM_MANAGER_DIR/helpers/tests/pacemaker/stop_vm.py

id "SEAPATH-00073" as "Test start VM" cukinia_cmd python3 $VM_MANAGER_DIR/helpers/tests/pacemaker/start_vm.py

id "SEAPATH-00074" as "Test remove VM" cukinia_cmd python3 $VM_MANAGER_DIR/helpers/tests/pacemaker/remove_vm.py

# --- vn manager pacemaker tests ---

cukinia_log "$(_colorize yellow "--- Test vm_manager module Ceph RBD part ---")"

id "SEAPATH-00306" as "Test clone disk" cukinia_cmd timeout -k 30s 30s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/clone_rbd.py

id "SEAPATH-00307" as "Test groups" cukinia_cmd timeout -k 30s 30s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/create_rbd_group.py

id "SEAPATH-00308" as "Test namespaces" cukinia_cmd timeout -k 30s 30s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/create_rbd_namespace.py

id "SEAPATH-00309" as "Test metadata" cukinia_cmd timeout -k 30s 30s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/metadata_rbd.py

id "SEAPATH-00060" as "Test snapshots" cukinia_cmd timeout -k 60s 60s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/purge_rbd.py

id "SEAPATH-00061" as "Test snapshots rollback" cukinia_cmd timeout -k 40s 40s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/rollback_rbd.py

id "SEAPATH-00062" as "Test write rbd" cukinia_cmd timeout -k 30s 30s python3 $VM_MANAGER_DIR/helpers/tests/rbd_manager/write_rbd.py
